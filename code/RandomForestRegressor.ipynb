{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d354da73",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import math\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5cd6f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc268c8",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce34b8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8760 entries, 0 to 8759\n",
      "Data columns (total 37 columns):\n",
      " #   Column                                                                         Non-Null Count  Dtype  \n",
      "---  ------                                                                         --------------  -----  \n",
      " 0   start MTU (UTC)                                                                8760 non-null   object \n",
      " 1   Day-ahead Price [EUR/MWh] BZN|NO1                                              8760 non-null   float64\n",
      " 2   Day-ahead Price [EUR/MWh] BZN|NO3                                              8760 non-null   float64\n",
      " 3   Day-ahead Price [EUR/MWh] BZN|NO5                                              8760 non-null   float64\n",
      " 4   Day-ahead Price [EUR/MWh] BZN|SE3                                              8760 non-null   float64\n",
      " 5   Actual Total Load [MW] - BZN|NO5                                               8760 non-null   float64\n",
      " 6   Hydro Run-of-river and poundage - BZN|NO1                                      8760 non-null   float64\n",
      " 7   Hydro Water Reservoir - BZN|NO1                                                8760 non-null   float64\n",
      " 8   Wind Onshore - BZN|NO1                                                         8760 non-null   float64\n",
      " 9   Hydro Pumped Storage Aggregated- BZN|NO2                                       8760 non-null   float64\n",
      " 10  Hydro Run-of-river and poundage - BZN|NO2                                      8760 non-null   float64\n",
      " 11  Hydro Water Reservoir - BZN|NO2                                                8760 non-null   float64\n",
      " 12  Waste - BZN|NO2                                                                8760 non-null   float64\n",
      " 13  Wind Onshore - BZN|NO2                                                         8760 non-null   float64\n",
      " 14  Hydro Pumped Storage Aggregated- BZN|NO3                                       8760 non-null   float64\n",
      " 15  Hydro Run-of-river and poundage - BZN|NO3                                      8760 non-null   float64\n",
      " 16  Hydro Water Reservoir - BZN|NO3                                                8760 non-null   float64\n",
      " 17  Other renewable - BZN|NO3                                                      8760 non-null   float64\n",
      " 18  Wind Onshore - BZN|NO3                                                         8760 non-null   float64\n",
      " 19  Fossil Gas - BZN|NO5                                                           8760 non-null   float64\n",
      " 20  Hydro Pumped Storage Aggregated- BZN|NO5                                       8760 non-null   float64\n",
      " 21  Hydro Run-of-river and poundage - BZN|NO5                                      8760 non-null   float64\n",
      " 22  Waste - BZN|NO5                                                                8760 non-null   float64\n",
      " 23  Hydro Water Reservoir - BZN|SE3                                                8760 non-null   float64\n",
      " 24  Nuclear - BZN|SE3                                                              8760 non-null   float64\n",
      " 25  Solar - BZN|SE3                                                                8760 non-null   float64\n",
      " 26  Wind Onshore - BZN|SE3                                                         8760 non-null   float64\n",
      " 27  CBF BZN|NO2 > BZN|NO1 [MW]                                                     8760 non-null   float64\n",
      " 28  CBF BZN|NO1 > BZN|NO2 [MW]                                                     8760 non-null   float64\n",
      " 29  CBF BZN|NO3 > BZN|NO1 [MW]                                                     8760 non-null   float64\n",
      " 30  CBF BZN|NO1 > BZN|NO3 [MW]                                                     8760 non-null   float64\n",
      " 31  CBF BZN|NO5 > BZN|NO1 [MW]                                                     8760 non-null   float64\n",
      " 32  CBF BZN|NO1 > BZN|NO5 [MW]                                                     8760 non-null   float64\n",
      " 33  CBF BZN|SE3 > BZN|NO1 [MW]                                                     8760 non-null   float64\n",
      " 34  CBF BZN|NO1 > BZN|SE3 [MW]                                                     8760 non-null   float64\n",
      " 35  Stored Energy Value Water Reservoirs and Hydro Storage Plants [MWh] - BZN|NO3  8760 non-null   float64\n",
      " 36  Stored Energy Value Water Reservoirs and Hydro Storage Plants [MWh] - BZN|SE3  8760 non-null   float64\n",
      "dtypes: float64(36), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../datasets/complete_data/df.csv')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a67dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max price of target feature = 654.81\n",
      "min price of target feature = 0.04\n",
      "mean price of target feature = 194.35976312785388\n",
      "meadian price of target feature = 170.03\n"
     ]
    }
   ],
   "source": [
    "print(\"max price of target feature =\", df[\"Day-ahead Price [EUR/MWh] BZN|NO1\"].max())\n",
    "print(\"min price of target feature =\", df[\"Day-ahead Price [EUR/MWh] BZN|NO1\"].min())\n",
    "print(\"mean price of target feature =\", df[\"Day-ahead Price [EUR/MWh] BZN|NO1\"].mean())\n",
    "print(\"meadian price of target feature =\", df[\"Day-ahead Price [EUR/MWh] BZN|NO1\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cdacf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58346f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48d31436",
   "metadata": {},
   "source": [
    "The datatype of the datetime column \"start MTU (UTC)\" is object, so we need to convert it into a datetime object. We then need to set the \"start MTU (UTC)\" column as the index, which is required for time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa3d2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to datetime\n",
    "df['start MTU (UTC)'] = pd.to_datetime(df['start MTU (UTC)'])\n",
    "\n",
    "# Set the date column as the index\n",
    "df.set_index('start MTU (UTC)', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b680239",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51290057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "# shuffle is set to false so that the ordinality of the data is maintained\n",
    "train, test = train_test_split(df, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea4dda",
   "metadata": {},
   "source": [
    "## Creating a lagged dataset with 24 lag features for each input feature\n",
    "\n",
    "### NB! this is direct copy, needs to be rewritten!\n",
    "When we want to make time series predictions, we need to consider the temporal relationship between the observations. In other words, we need to take into account the fact that each observation is measured at a particular time and that the observations over time are not independent of each other. Therefore, we can't simply use all of the data to train our model, as we would in a typical machine learning problem.\n",
    "\n",
    "Instead, we typically create a \"lagged\" dataset where each row contains the predictors and the target variable for a particular time step. We use this lagged dataset to train our model. To create this lagged dataset, we shift the target variable forward by one or more time steps and use the resulting series as the target variable for that time step. We also include the values of the predictors at the previous time steps as features. This way, the model can learn to use the past values of the predictors to predict the future value of the target variable.\n",
    "\n",
    "In the case of hourly data and a prediction horizon of 24 hours, we would create a lagged dataset where each row contains the predictors and the target variable for a particular hour. The target variable for each row would be the value 24 hours in the future, and the predictors for each row would be the values of the predictors for the previous 24 hours. We would then use this lagged dataset to train our model to make predictions for the next 24 hours.\n",
    "\n",
    "By shifting the target feature forward by one period, we are effectively creating a dataset where each observation includes the past observations up to the previous period and the target variable value for the next period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "543d0137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create lagged dataset wit 24 lag features for each input feature\n",
    "def create_lagged_dataset(df):\n",
    "    # creating a copy of the dataframe\n",
    "    lagged_df = df.copy()\n",
    "    \n",
    "    # Adding lagged features for target variable\n",
    "    lagged_df['Day-ahead Price [EUR/MWh] BZN|NO1'] = lagged_df['Day-ahead Price [EUR/MWh] BZN|NO1'].shift(-1)\n",
    "    \n",
    "    # Dropping the last row containing NaN values\n",
    "    lagged_df.dropna(inplace=True)\n",
    "    \n",
    "    # Creating a dataframe with lagged features with 24 steps for each of the original features\n",
    "    lagged_df = pd.concat([lagged_df.shift(i) for i in range(24)], axis=1)\n",
    "    \n",
    "    # Removing the NaN rows that have been created in the beginning of the dataset\n",
    "    lagged_df.dropna(inplace=True)\n",
    "    \n",
    "    return lagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35b7543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lagged dataset for train and test dataset\n",
    "lagged_train = create_lagged_dataset(train)\n",
    "lagged_test = create_lagged_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d6012b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the target feature and the input features\n",
    "train_y = lagged_train['Day-ahead Price [EUR/MWh] BZN|NO1']\n",
    "train_x = lagged_train.drop(['Day-ahead Price [EUR/MWh] BZN|NO1'], axis=1)\n",
    "\n",
    "test_y = lagged_test['Day-ahead Price [EUR/MWh] BZN|NO1']\n",
    "test_x = lagged_test.drop(['Day-ahead Price [EUR/MWh] BZN|NO1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest regressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "pred_y = model.predict(test_x)\n",
    "\n",
    "# Calculate the mean absolute error (MAE) between the predicted and actual values\n",
    "mae = mean_absolute_error(test_y, pred_y)\n",
    "print(\"Mean absolute error:            \", mae)\n",
    "\n",
    "# Calculate the mean absolute percentage error (MAE) between the predicted and actual values\n",
    "mape = mean_absolute_percentage_error(test_y, pred_y)\n",
    "print(\"Mean absolute percentage error: \", mape)\n",
    "\n",
    "# Calculating the mean squared error (MSE) between the predicted and actual values\n",
    "mse = mean_squared_error(test_y, pred_y)\n",
    "print(\"Mean squared error:             \", mse)\n",
    "\n",
    "# Calculating the root mean squared error (RMSE) between the predicted and actual values\n",
    "rmse = math.sqrt(mse)\n",
    "print(\"Rood mean squared error:        \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad09c96",
   "metadata": {},
   "source": [
    "### NB! this is direct copy, needs to be rewritten!\n",
    "In terms of MAPE, a commonly used benchmark is a value less than 10%. However, this can vary depending on the specific industry and application. For example, in finance, a MAPE less than 2% may be considered good.\n",
    "\n",
    "For RMSE, a good value depends on the scale of the data being analyzed. A common practice is to compare the RMSE to the range of the target variable. As a rough guideline, an RMSE that is less than 10% of the range of the target variable can be considered good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b62db1",
   "metadata": {},
   "source": [
    "## hyper parameter tuning\n",
    "\n",
    "When using the RandomForestRegressor from sklearn.ensemble, there are several hyperparameters that can be tuned to improve the performance of the model. Some of the most important hyperparameters are:\n",
    "\n",
    "n_estimators: The number of decision trees in the random forest. Increasing this parameter can improve the accuracy of the model, but may also increase the training time.\n",
    "\n",
    "max_depth: The maximum depth of each decision tree in the random forest. Increasing this parameter can allow the model to capture more complex relationships in the data, but may also increase the risk of overfitting.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node in a decision tree. Increasing this parameter can prevent overfitting, but may also decrease the accuracy of the model.\n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node in a decision tree. Increasing this parameter can also prevent overfitting, but may result in a simpler model with reduced accuracy.\n",
    "\n",
    "max_features: The maximum number of features considered for splitting each node in a decision tree. This parameter can affect the randomness of the forest and may impact the accuracy of the model.\n",
    "\n",
    "bootstrap: Whether or not to bootstrap the samples used for each tree. Setting this parameter to False can decrease the randomness of the forest and may increase the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ca4e1",
   "metadata": {},
   "source": [
    "This code uses GridSearchCV to perform a grid search over a range of hyperparameters, with 5-fold cross-validation to evaluate the performance of each combination of hyperparameters. The scoring parameter is set to 'neg_mean_squared_error' to indicate that the model should be optimized to minimize the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create a random forest regressor\n",
    "model = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "pred_y = best_model.predict(test_x)\n",
    "\n",
    "# Calculate the mean absolute error (MAE) between the predicted and actual values\n",
    "mae = mean_absolute_error(test_y, pred_y)\n",
    "print(\"Mean absolute error:            \", mae)\n",
    "\n",
    "# Calculate the mean absolute percentage error (MAE) between the predicted and actual values\n",
    "mape = mean_absolute_percentage_error(test_y, pred_y)\n",
    "print(\"Mean absolute percentage error: \", mape)\n",
    "\n",
    "# Calculating the mean squared error (MSE) between the predicted and actual values\n",
    "mse = mean_squared_error(test_y, pred_y)\n",
    "print(\"Mean squared error:             \", mse)\n",
    "\n",
    "# Calculating the root mean squared error (RMSE) between the predicted and actual values\n",
    "rmse = math.sqrt(mse)\n",
    "print(\"Rood mean squared error:        \", rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
